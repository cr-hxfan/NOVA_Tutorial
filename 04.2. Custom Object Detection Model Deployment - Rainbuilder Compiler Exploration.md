# Custom Object Detection Model Deployment (RainBuilder Compiler Exploration)



本教程旨在介绍RainBuilder编译工具中编译器`Compiler`的使用方法，以及模型在FPGA加速卡上的部署流程。

> 注意：本实验需要在**服务器**中的docker里进行。



本教程的安排如下：



- [1. Rainbuilder Compiler介绍与说明](#1-Rainbuilder-Compiler%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AF%B4%E6%98%8E)
- [2. RbCli的使用步骤](#2-RbCli%E7%9A%84%E4%BD%BF%E7%94%A8%E6%AD%A5%E9%AA%A4)
  - [2.1 冻结模型`RbCli freeze`](#21-%E5%86%BB%E7%BB%93%E6%A8%A1%E5%9E%8BRbCli-freeze)
  - [2.2 生成SG `RbCli sg`](#22-%E7%94%9F%E6%88%90SG-RbCli-sg)
  - [2.3 量化`RbCli quant`](#23-%E9%87%8F%E5%8C%96RbCli-quant)
  - [2.4 优化命令 `RbCli opt`](#24-%E4%BC%98%E5%8C%96%E5%91%BD%E4%BB%A4-RbCli-opt)
- [3. 模型部署](#3-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2)
  - [3.1 模型部署文件准备](#31-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%96%87%E4%BB%B6%E5%87%86%E5%A4%87)
  - [3.2 运行模型](#32-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B)
- [4. 练习：目标检测模型的编译和部署](#4-%E7%BB%83%E4%B9%A0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BC%96%E8%AF%91%E5%92%8C%E9%83%A8%E7%BD%B2)








预期目标：
1. 能够了解如何通过`RainBuilder`将模型部署至星空FPGA加速卡。
2. 熟练掌握编译器命令行工具**RbCli**的使用。






## 1. Rainbuilder Compiler介绍与说明
在《Custom Object Detection Model Deployment - Intro》中我们已经介绍了`Compiler`。`Compiler`通过命令行工具`RbCli`实现对网络模型进行解析和转换。

![SG flow.jpg](https://i.loli.net/2019/07/23/5d36ece8729fc16286.png)



`RbCli`命令由四个部分组成:`RbCli freeze`、`RbCli sg`、`RbCli quant`、`RbCli opt`，描述如下：

```bash

$ RbCli
Usage: RbCli [OPTIONS] COMMAND [ARGS]...

 RbCompiler CLI: a command-line tool for the RainBuilder Compiler.

Options:
--debug / --no-debug
-v, --version
--help                Show this message and exit.

Commands:
export  Export model coefficient from SG IR
freeze  Freeze a TensorFlow model from checkpoints
opt     Optimization SG from given SG IR
quant   Quant SG from given SG IR
sg      Generate SG IR from given frozen model

```







## 2. RbCli的使用步骤

在“目标检测模型训练教程”中，我们在`/lab_02`路径下得到了一个经过训练的推演模型`inference_model`。`inference_model`文件中的文件结构如下所示：

```bash

--inference_model
    |--ckpt.meta
    |--ckpt.data-00000-of-00001
    |--ckpt.index
    |--checkpoint
    
```



接下来我们将以该推演模型为基础，一步步讲解如何通过`compiler`将Tensorflow模型部署在星空FPGA加速卡上。



### 2.1 冻结模型`RbCli freeze`
#### 2.1.1 使用方式
`RbCli freeze`的使用方式如下，其中`[Model_dir]`为训练完成的模型的路径。
```bash

RbCli freeze [Model_dir] 

```


`inference_model` 为`RbCli freeze`的输入，输出为冻结模型文件`*.pb`，它定义了推理图的结构与各层神经网络的参数。



#### 2.1.2 命令描述

冻结命令`RbCli freeze`是将TensorFlow checkpoint路径下模型训练完成后的结果转化为冻结模型`*.pb`，一个训练完成的神经网络模型在`checkpoint`文件夹路径下应该包含以下三个文件：

1. `*.meta`: 模型的Meta信息。
2. `*.data*`: 训练数据文件。
3. `*.index`: 索引文件。

>**Note：**
>在模型训练过程中，TensorFlow会保存所有阶段性训练的结果。在进行冻结模型前，需要在checkpoint文件中只保留其中一个训练结果，不能将所有保存下来的训练结果都进行freeze。



#### 2.1.3 命令参数介绍
`RbCli freeze`的具体描述如下：
```bash
    
    $ RbCli freeze --help
    
	Usage: RbCli freeze [OPTIONS] MODEL_DIR

  	  Freeze a TensorFlow model from checkpoints

	Options:
  	-m, --model-file PATH         Freeze model file path with .pb suffix. If it
                                  is not specified, default generated in the
                                  current directory, name model.pb
  	-o, --output-node-names TEXT  Output node names, comma separated. If it is
                                  not specified, the default by RbCompiler to
                                  find and then you choose.
  	--logdir PATH                 Save the log directory for viewing tensorboard
  	--help                        Show this message and exit.

```

- ``MODEL_DIR``：必需参数，代表模型文件的存放路径。

- ``--model-file``: 可选参数, 用于指定输出的`*.pb`文件的名称，如果不指定具体名称, 会在当前执行命令的路径下生成一个名称为 ``model.pb``的文件。
- ``--output-node-names``: 可选参数，定义指定冻结的节点，如果需要多个输出节点，需要输入输出的节点名称并以逗号分开。如果该参数没有定义, `RbCli`会自动分析模型并找出有可能的输出节点并选择他们。



由于`RbCli`可以自动搜索输出节点，在运行`RbCli freeze`时不要指定节点名称或序号，让编译器自动完成输出节点检测。只有当特定的输出节点没有找到，或者某一个节点名称出现错误时，再使用指定节点输出功能。



### 2.2 生成SG `RbCli [frontend]`
需要注意的是，RbCcompiler的前端支持TensorFlow、Caffe和ONNX三种框架的解析，因此，针对不同框架，我们需要使用不同的命令。

```bash

# TensorFlow 
RbCli [OPTIONS] tf [ARGS]... 

# Caffe 
RbCli [OPTIONS] caffe [ARGS]... 

# ONNX 
RbCli [OPTIONS] onnx [ARGS]...

```



#### 2.2.1 使用方式

使用`RbCli`生成SG的方式如下：
```bash

RbCli [front-end-op] [Model_file]

```


 该命令以上一步的冻结模型文件`.pb`为输入，生成以`*_sg.pbtxt`结尾的模型结构文件和`*_sg.h5`结尾的模型参数文件，这两个文件是一种中间表达式，我们称之为**SG IR**.



#### 2.2.2 命令描述

该命令以上一步的冻结模型`*.pb`文件作为输入，生成**SG IR**文件，**SG IR**是流图的中间表达形式。流图主要由两部分构成：

1. 模型结构文件：使用`protobuf`协议，保存在以`.pbtxt`为后缀的文件中。
2. 模型参数文件：使用`h5py`格式保存在以`.h5`为后缀的文件中



#### 2.2.3 命令参数介绍

该指令的具体描述如下:
```bash
    
    $ RbCli tf --help

	Usage: RbCli tf [OPTIONS] MODEL_FILE

      Generate SG IR from given tf frozen model

	Options:
 	-n, --model-name TEXT           Model name.
  	-o, --output-dir PATH           Output directory, save generated SG IR.
  	-s, --input-image-shape TEXT    A default shape of the input image for the
                                    model.
  	-f, --data-format TEXT          One of channels_last or channels_first,
                                    default channels_last, like 'NHWC' is
                                    0,2,3,1.
  	-c, --custom-layer-config PATH
  	--with-json BOOLEAN             Whether to export the json file of SG IR,
                                    default False
  	--with-coeff BOOLEAN            Whether to export model coefficients such as
                                    weights and bais, default False
  	--help                          Show this message and exit.

    
```
- `-n` 用于修改`*.pb`文件的名称。
- `-o` 用于指定输出的模型描述文件的存放路径，若不指定，RbCli将会在把输出的文件保存当前目录下。
- `-s` 用于指定模型输入图片的形状，如1,256,256,3。如果输入的模型shape是动态变换的，使用该参数设置特定的shape。
- `-f` 选择数据的shape的排列方式，可以输入`0,1,2,3`，1代表batch size，2代表channel，3代表height，4代表width。
- `-c` 用于指定用户模型中自定义层配置文件(json)的路径。
- `-v` 用于为`*.pbtxt`设置版本号，可以输入`V1`或者`V2`，默认为`V2`。
* ``--with-json``: 导出json文件，默认为`false`。
* ``--with-coeff``:  设定是否导出模型的参数比如权重`weights`和偏置`bias`。



### 2.3 量化`RbCli quant`

#### 2.3.1 使用方式
`RbCli quant`命令的使用方法如下：

```bash

RbCli quant [*.pbtxt] [*.h5] [preprocess_script] \

            --preprocess-range -123, 151 \
            
            --img-dir [Database_path] \
            
            --output-dir [Frozen_Model_path] \
            
            --with-coeff True
            
            --gpu-config [gpu_config file]
            
```



`*_sg.pbtxt` 和 `*_sg.h5` 是该命令的输入，在此我们提供了`fddb`数据库对模型进行量化，该数据库的路径为`/lab_04.1/fddb`，同时，我们也提供了一个针对`fddb`数据库中的图像进行预处理的脚本 `preprocess_ssd.py`。数据库与脚本的路径均为 `/lab_04.1/`。



#### 2.3.2 命令描述

可以使用`RbCli quant`对**SG IR**进行8-bit量化操作。在执行此操作前，用户需要提前准备一定量的实际数据，数据应尽量与目标场景中的实际数据相似，并且使用python作为数据的处理方式。



#### 2.3.3 命令参数介绍

`RbCli quant`命令的详细描述如下：


```bash
$ RbCli quant --help

Usage: RbCli quant [OPTIONS] SG_DEF_FILE SG_H5_FILE PREPROCESS_PY

  Quant SG from given SG IR

Options:
  --use-kld BOOLEAN         Whether to use KLD quantization, default False.
  --float-coeff PATH        Folder path for model coefficients.
  --img-dir PATH            Input data set directory path used for SG
                            quantization, default current directory.
  --preprocess-range TEXT   Theoretical input range of the model after pre-
                            processing. the format is 'min,max', defalut is
                            0,255
  --output-dir PATH         Output directory, save Quant SG IR.
  --with-sim BOOLEAN        Save the SG IR used by the raintime sim model,
                            default False
  --with-sim-align BOOLEAN  Save the SG IR used by the raintime sim model and
                            open model channel 64 alignment, default False
  --with-fakequant BOOLEAN  Save quantified results, default False
  --with-coeff BOOLEAN      Whether to export model quant coefficients,
                            default False
  --gpu-config PATH         GPU configuration file.
  --help                    Show this message and exit.

```

- ``--preprocess-range`` 在对输入图片进行前处理之后，理论上的模型输入值范围，该参数指定最小值与最大值`min,max`, 默认的输入范围是0,255。
- ``--img-dir``: 在进行**SG IR**量化操作所需要输入数据路径。
- ``--output-dir``: 量化结果的输出路径。
- ``--with-coeff``: 设定是否导出模型的参数比如权重`weights`和偏置`bias`。
- ``--gpu-config``: GPU配置文件，使量化操作在GPU上运行。由RainBuilder Compiler提供配置文件。
- ``--with-sim``: 调试参数，实际使用中无需特别指定。
- ``--with-fakequant``: 调试参数，实际使用中无需特别指定。



### 2.4 优化命令 `RbCli opt`

#### 2.4.1 使用方式
`RbCli opt`命令的使用方式如下，其中`opt_config`为板卡的描述文件，该文件用于分配模型中不同节点的运算执行设备。
```bash

RbCli opt [*_sg.pbtxt] [*_sg.h5] [opt_config] -o [Output_path] -c true

```



`opt_config` 为板卡的描述文件，用于为模型中的不同节点分配执行计算的硬件。`CAISA.pbtxt`为板卡的描述文件，其路径为 `/lab_04.1/`。



#### 2.4.2 命令描述

如果需要将神经网络模型部署在星空FPGA加速卡上执行，则需要生成匹配硬件计算资源的模型文件。所以，本步骤首先根据硬件的计算资源配置，将多个SGNode算子操作融合为一个计算操作，节省IO的时间损耗；另外，由于每个硬件板卡的计算资源和架构各有不同，不是所有SGNode都可以部署FPGA上实现硬件加速，有一些特殊计算操作需要分配到CPU上运行；最后，该步骤还会根据硬件加速板卡的硬件资源配置评估最佳的计算资源和并行度参数。



`RbCli opt`命令实现2种针对FPGA硬件的优化：

| 优化策略 | 功能                                     |
| -------- | ---------------------------------------- |
| Device   | 为SG节点分配执行设备（FPGA或CPU）        |
| Fusion   | 将多个算子融合为一个以提高资源和计算效率 |



#### 2.4.3 命令参数介绍

`RbCli opt`命令的详细描述如下:


```bash

$ RbCli opt --help

Usage: RbCli opt [OPTIONS] SG_DEF_FILE SG_H5_FILE OPT_CONFIG

  Optimization SG from given SG IR

Options:
  -c, --ch-align BOOLEAN  if True, open model channel 64 alignment.
  -o, --output-dir PATH   Output directory, save Opt SG IR. If not specified,
                          the default is to the directory where the
                          sg_def_file file is located.
  --help                  Show this message and exit.
  
```

- `SG_DEF_FILE`: 量化后的模型结构。
- `SG_H5_FILE`: 模型参数。
- `OPT_CONFIG`: 板卡硬件参数定义文件。
- `o`：用于指定输出路径



至此，模型的结构描述文件`*_opt_sg.pbtxt`和模型的参数文件`*_quant_coeff/coeff_little`生成完毕，通过在FPGA上部署这两个文件，便能够实现对该模型的加速。







## 3. 模型部署

### 3.1 模型部署文件准备

SSD模型在星空FPGA加速卡上部署需要三个模型相关文件，分别为：
- `post_param`，后处理文件，该文件在"目标检测模型训练教程"中生成，存放路径为`./lab_02`
- `*_opt_sg.pbtxt`，量化并优化后的模型结构文件，存放路径为`./lab_04/lab_04.1/`
- `coeff_little`，量化并优化后的模型参数文件，存放路径与结构文件一致，为``./lab_04/lab_04.1/`

为了方便，我们需要将这些文件统一放置在一个目录下如`./lab_04/lab_04.1/object_detection/ssd`，因此，我们得到的文件结构为：


​    
```bash

--ssd
    |--ssd_opt_sg.pbtxt
    |--ssd_config.txt
    |--post_param
    |--ssd_quant_coeff
    |   |--coeff_little
    
```





### 3.2 运行模型

进入`ssd`文件夹，通过执行如下指令运行模型。


```bash

./image_object_detection \							#执行可执行文件

  --sg ./ssd_opt_sg.pbtxt \							#模型结构文件
  
  --images ../fddb/testing-data/ \					#测试图片
  
  --data ./ssd_quant_coeff/coeff_little/ \			#模型参数
  
  --processor ssd \
  
  --batch 1 \
  
  --group 1000 \
  
  --num_threads 2 \
  
  --image_list ../references/fddblist.txt \
  
  --output_file ../predictions.txt \
  
  --from_file=true \
  
  --config_file ../ssd_config.txt \					#SSD后处理配置文件
  
  --param_path ./poat_param/ \						#后处理（anchor）参数
  
  --with_image_output=false

```

>**Note：**
>可以发现，我们使用了一个名为`image_object_detection`的可执行程序，并向其输入了相应的参数以执行模型。`image_object_detection`是我们预先编译好的runner程序，该程序基于`RbRuntime`开发。我们将会在《Custom Object Detection Model Deployment - RainBuilder Runtime Exploration》中详细介绍具体的开发方法。







## 4. 练习：目标检测模型的编译和部署

在实验《Neural Network Design with TensorFlow》中，我们得到了已经固定输入的目标检测模型的checkpoint文件。



本实验要求：

1. 用户使用`Compiler`完成模型的编译。编译完成时应该得到模型的结构描述文件`*_opt_sg.pbtxt`和模型参数文件`*_quant_coeff/coeff_little`。
   - 
2. 将编译完成的模型文件部署到FPGA上，并执行模型。
