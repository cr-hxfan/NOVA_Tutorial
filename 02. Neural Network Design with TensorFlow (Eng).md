# Neural Network Design with TensorFlow



This tutorial introduces the basic concept of VGG network, principle of SSD object detection framework, and implement a complete backbone network for CNN.



The tutorial is organized as follow,



- [1. Preliminary](#1-Preliminary)

    - [1.1 VGG Network](#11-VGG_network)

    - [1.2 SSD Object Detection Framwork](#12-SSD_Object_Detection_Framwork)

- [2. Exercise](#2-Exercise)

    - [2.1 Building a SSD Neural Network based on VGG structure](#21-Building_a_SSD_Neural_Network_based_on_VGG_structure)

    - [2.2 Network Training](#22-Network_Training)







Goal：

1. To understand the structure and feature of VGG, and the principle of the mainstream objet detection framework SSD

2. To build a neural network using Tensorflow







## 1. Preliminary

### 1.1 VGG Network

[VGG](https://arxiv.org/abs/1409.1556) is proposed by Visual Geometry Group, Oxford University. A standard VGG network is shown below. In VGG, all filters in convolutional layer is of size 3x3, size of pooling is 2x2.

![VGG](https://i.loli.net/2019/07/04/5d1d6ff94747420902.png)



- One key feature of VGG is having small filters. VGG cascades multiple 3x3 filters to realise the reception field of a larger filter of size 5x5 or 7x7. This can effectively reduce the number of parameters, while provide a more non-linear mapping, improving the ability of the network to fit the data. 



- Another obvious feature of VGG is having more channels. The first layer of VGG has 64 channels, and the number of channels doubles every layer to a maximum of 512. As the number of channel increases, more information can be extracted



VGGNet explores the relation between the depth of the convolutional neural network and its performance. It successfully builds a 16-19 layer CNN, proving that increasing the depth of network can improve the final performance to a certain extent. It greatly reduces the error rate and can be generalised to other image data. Therefore, VGG is often used for image feature extraction, being the backbone network of other deep learning models.



### 1.2 SSD Object Detection Framework

With the advancement of image feature extraction networks, there is a great improvement in image-related tasks, such as object detection.



There are 2 types of common object detection algorithms, two-stage and one-stage.



Two-stage detection is split into 2 steps. Firstly, the model proposes a set of regions that may contain the object (region proposal). Then, the model extracts the feature of the candidate regions for classification and a more accurate location (regression).



Instead of splitting the process into 2 steps, one stage detection can finish the inference by passing the image to the network once. It greatly improves the detection speed while maintaining a similar accuracy compared to two-stage detection, making it more suitable for industrial applications.



This experiment uses the classic one-stage detection method Single-Shot Multibox Detector (SSD). Principle of SSD can be found in the [original paper](https://arxiv.org/abs/1512.02325)。



A standard SSD network mainly consists of a backbone network (feature extraction network) and multi-scale detection branches. 

![ssd.jpg](http://images2015.cnblogs.com/blog/1005315/201703/1005315-20170321214539611-212225585.png)



SSD network uses VGG-16 for image feature extraction. In VGG, towards deeper layers, the size of the feature map reduces. Feature map in different layer represents semantic information of different scales. Shallow feature maps include more partial features, which tends to describe the detail of the image, so it is more suitable for small scale object detection. Deep feature maps capture the more abstract global information, which is suitable for large scale detection.



SSD uses multiple detection branches for prediction with feature maps of different scales. Each detection branch is responsible for predicting the class and location of the bounding box of the respective scale. Anchors are used for prediction in the detection branches. Anchor includes multiple pre-defined default bounding boxes. SSD is used to fine-tune the default bounding boxes to achieve a high accuracy detection result.



The concept of Anchors in SSD is shown in the following figure,

![example.PNG](https://i.loli.net/2018/11/29/5bff52b8e1cd7.png)



The above diagram shows the process of identification of cats and dogs in SSD, with blue represents the cat and red represents the dog. As the size of the cat and the dog are different in the image, SSD uses different branches for the detection of cat and dog. 



Figure（b）shows the branch for 8x8 feature map, while figure（c）shows the branch for 4x4 feature map.（b）is in a shallower layer, corresponding to the detection of cats.（c）is located in a deeper layer, corresponding to the detection of dogs. The frames with dashed lines in figure（b）,（c）represent the bounding boxes for Anchor. Bounding boxes have different sizes and aspect ratio to ensure they can match objects with different sizes and shapes. Based on the matched Anchors, SSD calculates the location deviation Δ(*cx, cy, w, h*) of the region represented by the anchor and the corresponding classification confidence (*c_1, c_2, ..., c_p*), to locate and classify the objects.







## 2. Exercise

The experiment in this tutorial is split into 2 parts. The first part is to build a backbone network using the SSD framework. The second part is model training and preparation of the model for actual applications. Model post-processing is not done in this tutorial, detailed introduction and implementation of post-processing will be included in《Custom Object Detection Model Deployment》.



All files for this experiment are stored under `./lab_02`.



### 2.1 Building an SSD Neural Network based on VGG structure

In《Introduction to TensorFlow》, we have familiarised ourselves with the `slim` module. Therefore, `slim` is used to build the SSD network in this experiment. The structure is shown as follow, 

![SSD](https://i.loli.net/2019/07/22/5d3584edddf9e49336.png)



We use the feature map generated by block3, block4, block7, block8, block9, block10 for prediction. Thus, we recommend users to label the output of the corresponding layer block3, 4, 7, 8, 9, 10 when defining the model.



Experiment requirments：

1. From line 440 of `/lab_02/nets/ssd_KYnet_v3_6b_exercise.py`, use `slim` to implement the network structure shown above
2. Name the corresponding output layer correctly as required
3. From line 536 of `/lab_02/nets/ssd_KYnet_v3_6b_exercise.py`, define the parameters of convolution and pooling of the model

   - use `SAME` for paddnig in all calculations, set `data_format` to `NHWC`
   - All convolution filters are of size 3×3, use `caffe_scope.conv_weights_init()` for weight_regularizer, ` caffe_scope.conv_biases_init()` for biases_initializer, use `tf.nn.relu` as activation function
   - Use max pooling for pooling layers, with pooling filter size of 2x2

After you have completed the network construction, save your file as `ssd_KYnet_v3_6b.py` in the same directory using the following command.

``` shell
cp ssd_KYnet_v3_6b_exercise.py ssd_KYnet_v3_6b.py
```


### 2.2 Network Training

After building the target model, we need to train it. The purpose of this experiment is to obtain the final two files `inference_model` and `post_param`.

> Note: This experiment is recommended in docker.



In `./lab_02`, we provided 4 python files `tf_convert_data.py`, `train_ssd_network.py`, `post_gen.py`, and `export_inference_graph.py`, to assist users in this experiment. The 4 files are：

- `tf_convert_data.py`：Converting the training data to TFrecord format

- `train_ssd_network.py`：Training the model

- `post_gen.py`：Exporting the default parameter of anchor based on the model

- `export_inference_graph.py`：Removing checkpoints that are irrelevant to inference, keeping the core computation checkpoints



1）We provided 19 images for training to enhance users' understanding of the training process. Data is labelled as follow：

```

[Name of image] [number of objects] [Object 1]:[label_id of object 1] [2(predefined field)] [x coordinate of top left corner] [y coordinate] [x coordinate of bottom right corner] [y coordinate]

```

First, we need to convert the data format,

```python

python tf_convert_data.py \                  #target python file

       --dataset_name=test_release \         #name of dataset

       --dataset_dir=./imagetxt/ \           #input path

       --output_dir=./data_conversion \      #output path

       --shuffle=True

```



2）Then, we train the model. We will get the checkpoint file during training.

```shell


mkdir ./logs_test/

cp ./data_conversion/test_release.json ./logs_test/image.json

cp ./data_conversion/test_release.INFO ./logs_test/image.INFO



python -u train_ssd_network.py \

      --gpus=0 \                            #id of the GPU used for computation

      --num_clones=1 \                      #number of GPU used for training

      --train_dir=./logs_test/ \            #indicate the path for the checkpoint file

      --dataset_dir=./data_conversion \        

      --dataset_name=test_release \	        #name of the dataset in TF format

      --dataset_split_name=train \

      --model_name=ssd_KYnet_v3_6b \        #model

      --save_summaries_secs=600 \

      --save_interval_secs=1200 \           #interval for saving the model file

      --weight_decay=0.00001 \              #weight decay

      --optimizer=adam \

      --learning_rate=0.005 \               #initial learning rate

      --learning_rate_decay_factor=0.95 \   #learning rate decay factor

      --batch_size=20  \

      --debug_type=True \

      --num_epochs_per_decay=100.0          #number of epochs to adjust learning rate

```



3）After training the model, we can use `post_gen.py` to export the default parameters of the anchors of the model. This is to reduce the complexity of the model：For real-life application, evaluation can be speeded up by only running core computation on the accelerator, and use CPU for parts that are not related to inference. 

```python

python post_gen.py \        #target python file

       ssd_KYnet_v3_6b \    #state the network structure

       ./post_param         #output path

```



4） Finally, export the model. The main purpose is to fix the size and format of the input data. As the model is still in training state after training is finished, if the data input is still in a queue, input `shape` of `batch_size` is still relatively large. Therefore, before compiling the model using `compiler`,  we have to pre-process the model, so that the model can be correctly compiled by the `compiler`.

```shell

python export_inference_graph.py \  #target python file

       ssd_KYnet_v3_6b \

       ./logs_test \                #input path

       ./inference_model            #output path

```



