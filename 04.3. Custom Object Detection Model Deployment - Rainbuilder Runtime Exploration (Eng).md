 # Custom Object Detection Model Deployment - Rainbuilder Runtime Exploration



This experiment provides a more detail introduction to `RbRuntime` and model data pre- and post-processing. In this tutorial, we introduce the more common API, and use these API to build a model suitable for CAISA architecture. Pre-processing for typical evaluation tasks and post-processing of model output data are also introduced in this tutorial. 



The tutorial is organised as follow,



- [1. Introduction to `RbRuntime`](#1-Introduction-to-rbruntime)

- [2. Build a Runner Program](#2-Build-a-runner-program)

    - [2.1 Create an SG object](#21-Create-an-sg-Object)

    - [2.2 Load Model Parameters](#22-Load-Model_parameters)

    - [2.3 Create Runner](#23-Create-runner)

    - [2.4 Run the Runner](#24-Run-the-runner)

- [3. Understanding pre-processing and post-processing](#3-Understanding-pre-processing-and-post-processing)

    - [3.1 Image Pre-processing](#31-Image-Pre-processing)

    - [3.2 Model Post-porcessing](#32-Model-Post-processing)

        - [3.2.1 Processing for Model Output Data](#321-Processing-for-Model-Output-Data)

        - [3.2.2 Introduction to NMS](#322-introduction-to-nms)

- [4. Compiling the `Runner`](#4-PCompiling-the-runner)

- [5. Exercise](#5-Exercise)

    - [5.1 Implementing Image Pre-processing](#51-Implementing-Image-Pre-processing)

    - [5.2 Data Post-processing](#52-Data-Post-processing)

    - [5.3 Run the experiment](#53-Run-the-experiment)







Goal：

1. To understand how to use the RbRuntime API to build a Runner Program

2. To implement typical image pre-processing for inference

3. To implement post-processing of model output data for SSD object detection framework







## 1. Introduction to `RbRuntime`

RainBuilder Runtime, `RbRuntime`, is a library for CNN computation on FPGA systems. RbRuntime is used to run the SG generated by `RbCompiler`. According to the SG description, computation is done and the data for computation nodes are initialized by the coefficient file. 



`RbRuntime` provides the information for every node according to the SG description, distributing different computation functions, data types, and computing resources to different nodes.



`RbRuntime` also provides a simple I/O. Only a few lines of code is needed to incorporate the computing function of `RbRuntime` to target applications.



The major procedure of using `RbRuntime` is as follow, 

>1. Load pbtxt to SGDef Object，use SGBuilder to generated SG from SGDef
>2. Load the model parameters to SGDataMap
>3. Create SGRunner
>4. Prepare the input data for SGDataMap，then use Run or RunAsync in SGRunner to start calculations. Output SGDataMap containing the result will be generated. 

![](https://i.loli.net/2019/07/05/5d1ebc7dc124b27139.png)



## 2. Build a Runner Program

In the last experiment《Rainbuilder Compiler Exploration》, we used `compiler` to finish the analysis and export of model data for SSD object detection framework, and got the following model file. 

```shell

/object_etection
  |-image_object_etection
  |-run.sh
  /images_ssd
    |-img_55.jpg
    |-...
  /ssd
    |-ssd_config.txt
    |-ssd_opt_sg.pbtxt
    /post_param
      |-post_anchor_0.bin
      |-...
    /quant
      /float_little
      |-config.json
      |-...

```

In this experiment, we use `/lab_04.2/object-detection/image_object_detection.cc` as an example.



In `image_object_detection.cc`, we use `ImageObjectDetection` to manage the data and process when running the model. 



The following is the main function of `image_object_detection.cc`：

```c++

ImageObjectDetection img_detect{FLAGS_sg, FLAGS_data, FLAGS_num_threads,
                                FLAGS_batch, (float)FLAGS_threshold};



ImageProcessorFactory factory;

//read SSD parameters
factory.SetConfigFile(FLAGS_config_file);

//set SSD post-processing procedure
factory.SetFlag(FLAGS_processor);

//read SSD anchor related information
factory.SetParamPath(FLAGS_param_path);

img_detect.BatchRun(FLAGS_images, factory);

```



Definition of `ImageObjectDetection` is as follow：

```c++

  ImageObjectDetection(std::string const &sg_def_file,
                       std::string const &data_dir, int num_threads,
                       int input_batch, float threshold) {
      
    this->sg_def_ = LoadSGDefFromFile(sg_def_file.c_str());
    
    this->sg_ = BuildSG(this->sg_def_);
      
    this->const_data_ = LoadSGDataMapFromDir(data_dir.c_str());
      
    this->input_node_def_ = FindImageInput(this->sg_def_);
      
    this->runner_ =
        BuildSGRunner(this->sg_, this->const_data_, num_threads, input_batch);
      
    this->threshold = threshold;
  }

```



### 2.1 Create an SG object

RbRuntime provides the following API to load the pbtxt to SGDef data structure during initialization, and convert the data in SGDef to SG object.

```c++

this->sg_def = LoadSGDefFromFile("/object_etection/ssd/ssd_opt_sg.pbtxt");

this->sg = BuildSG(this->sg_def);

```



In the provided `image_object_detection.cc`, the corresponding lines of code are ：

```c++

this->sg_def_ = LoadSGDefFromFile(sg_def_file.c_str());

//load the file for model structure definition, input parameter is the path to the model structure file, pbtxt
this->sg_ = BuildSG(this->sg_def);

```



### 2.2 Load Model Prarmeters

Use the API provided by `RbRuntime` to load the model parameters and input the information for the nodes：

```c++

this->const_data = LoadSGDataMapFromDir("/object_etection/ssd/quant/float_little");

this->input_node_def = FindImageInput(this->sg_def);

```

In the provided `image_object_detection.cc`, corresponing lines of codes are：

```c

//load model parameter file, input parameter is the path for the model parameter file
this->const_data_ = LoadSGDataMapFromDir(data_dir.c_str());

//Confirm input modes
this->input_node_def_ = FindImageInput(this->sg_def);

```



### 2.3 Create Runner

Create SGRunner, specify number of threads as num_threads, and execute pre-processing and post-processing in `img_detect.BatchRun()`,  we will have a further discussion about pre-processing and post-processing in this tutorial.

```c++

this->runner_ = BuildSGRunner(this->sg_, this->const_data_, num_threads, input_batch);

```



### 2.4 Run the Runner

Finally, run the Runner to start the calculation:

```

auto output_data_map = runner->Run(sg, const_data, input_data_map);

```



In the provided `image_object_detection.cc`, the corresponding lines of code are：

```

runner->RunAsync(sg_, const_data_, pair.second);

```







## 3. Understanding Pre-processing and Post-processing

As the formats and types of data acceptable to the algorithm model are fixed, before executing the model, data need to be processed into the format and type recognisable to the network. Before running an image classification or object detection algorithm, we need to make sure the size of the input image matches the requirement of the SG(*.pbtxt), so we have to pre-process the images.



After the execution of SG in RbRuntime, we will get one or a few output tensors. To make the result more obvious, for classification, we return the classification label; for object detection, the objects are identified by frames around them. Therefore in post-processing, computation are done to the output tensor, confirming the size and location of the frame and put the frame around the object, or map the output tensor to a classification label. 



### 3.1 Image Pre-processing

In this experiment, we provided 256x256 RGB image as the input for the SSD model. Therefore, the pre-processing includes the following steps,

1. Read the image

2. Rearrange the order of the channel from BGR to RGB

3. Reduce the size of the image to 256x256

4. Subtract the average from the images，R channel -123.0，G channel -117.0， B channel -104.0 

   > (1) Why we have to rearrange the order of channel？
   >
   > - The image loaded from cv::Mat is of BGR. In training, the images are of RGB format. To ensure the correctness of the evaluation result, we have to rearrange the channel in the image pre-processing.
   >
   >
   > (2) Why we have to subtract the average from the images？
   >
   > - In training, if the input data are all positive instead of distributed with zero as the centre, when using the ReLU as activation function, activating value for all layers will be positive. The gradient for a particular layer would be either all positive or all negative for back-propagation. The optimization path of the target function for the gradient descent would be an unfavourable z-shape, worsen the convergence of the algorithm. So we have to shift the data to distribute around 0. The same type of data is used in inference as well. -123.0，-117.0，-104.0 are the VGG-16 official average value for the RGB channels. 



In this experiment, image pre-processing is implemented using `PreProcess()` in  `image_object_detection.cc`.



### 3.2 Model Post-processing 

#### 3.2.1 Processing for model output data

When defining a SSD network, anchors (reference frames) are also defined. SSD network does not locate the exact location of the object, but calculates the relative shift between the object and the anchor. If the centre, height, and width of the bounding box are (x, y, h, w), and an anchor with location, height, and width of (xa, ya, ha, wa), then the SSD network output is (x’, y’, h’, w’), their relations are as follow.



> x’ = (x - xa) / wa；
>
>
>
> y’ = (y - ya) / ha;
>
>
>
> w’ = log(w / wa)；
>
>
>
> h’ = log(h / ha);



post-processing uses the output of SSD network, (x’, y’, h’, w’), as input, combining the anchor(xa, ya, ha, wa)，by **reversing the above equations**, it can compute the centre, height and width of the target frame (x, y, h, w).



In the last experiment, we built 6 feature maps of different scales to carry out object detection using SSD network (as seen below). The output layers are block3, block4, block7, block8, block9, and block10. Each output layer outputs feature maps of different scales, [(64, 64), (32, 32), (16, 16), (8, 8), (4, 4), (2, 2)]. From the default of the program, the numbers of anchors used in each region for each layer are 4, 4, 6, 6, 6, 4 respectively. Each anchor detects 2 types of objects, target and background.

![SSD](https://i.loli.net/2019/07/22/5d3584edddf9e49336.png)

model data post-processing refers to the processing of the model output data.



In the model `/lab_02/nets/ssd_KYnet_v3_6b.py`, we pass the feature map from each output layer to 2 different convolutions, `conv_loc` and `pred_loc`, resulting in 2 new feature maps for the target's location information `loc_pred` and class information `cls_pred`. They are used to produce the prediction frame.（see line 402-414 in `ssd_KYnet_v3_6b.py`）



In the next Runner procedure, from the `init_loc_cls()` function in `/lab_04/lab_04.2/object-detection/image_processor.cc`, we get the `loc_pred` and `cls_pred` feature maps from the node name of the output layer, and initialize the `cls` and `loc` arrays. （definition of `init_loc_cls()` can be seen from line 233 in `image_processor.cc`）



After getting the `cls` and `loc`, we can arrange them, and recover the 4 coordinates for each frame from `loc`, and use NMS to filter the excess frames.



#### 3.2.2 Introduction to NMS

Object detection algorithms usually output many detection frames. Most of these results are redundant, which some of the frames overlap a lot with each other and can be combined. We use the Non-Maximum Suppression (NMS) algorithm to eliminate the interference of multiple detection results. The goal of NMS is to remove excess detection frames and keep the best one. Effect of NMS is illustrated as follow,

<img src="https://i.loli.net/2018/12/08/5c0bd7fd9d8d6.png" style="zoom:75%" />



The principles of NMS are：

1. Arrange all the frames from highest confidence score to the lowest, choose the highest scoring frame

2. Traverse the rest of the frames, compute the IOU between each frame and the chosen frame from step 1. If it is greater than the threshold, remove the current frame

3. Choose the frame with highest confidence scores from the remaining frames, repeat the above process

```shell

# sudo code for NMS

for i in n:

	for j in i+1 … n:

		If IOU(box[i], box[j])>threshold

			Delete box[j];

```



IOU（Intersection over Union）computes the overlap between 2 frames.

<img src="https://i.loli.net/2018/12/08/5c0be1960c291.png" style="zoom:50%" />

```python

iou = interArea / (boxAArea + boxBArea - interArea)

# interArea Intersection of 2 frames
# boxAArea + boxBArea Total area of 2 frames

```



In this experiment, image post-processing is done using the `post_op()` function, as seen from line 172 to 204 in `image_processor.cc`.





## 4. Compiling the `Runner`



In `/lab_04/lab_04.2/object-detection`, we provided the corresponding `CMakeList.txt` to generate the makefile for compilation.



The compilation is very simple. After entering docker, execute the following code to compile an executable：

```shell

mkdir build

cd build

cmake .. 

make

```







## 5. Exercise

### 5.1 Implementing Image Pre-processing

Complete the image pre-processing from line 163 in `/lab_04/lab_04.2/object-detection/image_processor.cc`, requirements are as follow：

1. As image imported from cv::Mat is of BGR format, channel reordering is needed. Convert the BGR format to RGB format

2. Subtract the mean to every channel. Check line 155 of `mean_val` for the corresponding mean value. 

3. The returned data is 1-d array, arranged in RGB order



### 5.2 Data Post-processing

From line 31 in `/lab_04/lab_04.2/object-detection/third_party/libssd/post_process.cc`, complete the core function for `PostProcess()`. The function of `PostProcess()` is as follow：

1. Traverse all anchor for each layer, get the centre coordinate, height and width of every anchor, center_y, center_x, h, w

2. Use the anchor to convert the network output back to the centre coordinate, height and width of bounding boxes

3. Use the centre coordinate, height and width of from（2）as input, use NMS to filter redundant bounding boxes

4. Finally, return a vector of type bbox. Definition of `bbox` can be found in `/lab_04/lab_04.2/object-detection/third_party/libssd/bbox.hh`



### 5.3 Run the experiment

**According to the procedure in chapter 4《Compiling the `Runner`》, compile the Runner in docker and get the executable.**



Until this point, we have got the executable, model structure file (.pbtxt), quantized model parameter file (quant), and the provided configuration file (*_config.txt). This is all we need for model deployment. 



For simplicity, we put all files in a folder  (e.g. `./deployment`)

```shell

#enter the deployment folder
cd deployment

```



Use the following command to execute model acceleration, and Observe the output. 

```shell

 ./image_object_detection \						#Executable file we offerd

  --sg ./ssd_opt_sg.pbtxt \						#Model structure file
  
  --images ../fddb/testing-data/ \				#Test image
  
  --data ./ssd_quant_coeff/coeff_little/ \		#Model coefficient file
  
  --processor ssd \
  
  --batch 1 \
  
  --group 1000 \
  
  --num_threads 2 \
  
  --image_list ../references/fddblist.txt \
  
  --output_file ../predictions.txt \
  
  --from_file=true \
  
  --config_file ../ssd_config.txt \				#Postprocessing config file we offerd
  
  --param_path ./poat_param/ \					#post process parameter
  
  --with_image_output=true

```











