# Introduction to TensorFlow



This experiment briefly introduces Tensorflow and Convolutional Neural Networks (CNNs), starting from the basic elements of CNN, to the implementation of computation in Tensorflow, and finally building a simple CNN. 

> Note: Some parts of this tutorial are  recommended to do in docker.



This tutorial is organized as follows：



- [1. Introduction to CNN](#1-introduction-to-cnn)
    - [1.1 Introduction to TensorFlow](#11-introduction-to-tensorflow)

    - [1.2 Basic Elements of CNN ](#12-basic-elements-of-cnn)

        - [1.2.1 Convolution Layer](#121-convolution)

        - [1.2.2 Pooling Layer](#122-pooling)

        - [1.2.3 Activation Function](#123-activation-function)

        - [1.2.4 Fully Connected Layer](#124-fully-connected-layer)

    - [1.3 Implementing a Computational Layer](#13-implementing-a-computational-layer)

- [2 Introduction to CIFAR-10](#2-introductrion-to-cifar-10)

    - [2.1 Data Conversion](#21-data-conversion)

    - [2.2 Model Introduction](#22-model-introduction)

        - [2.2.1 Introduction to the `slim` Library](#221-introduction-to-the-slim-library)

        - [2.2.2 Common `slim` API](#222-common-slim-api)

    - [2.3 Data Pre-processing and Network Model Training](#23-data-pre-processing-and-network-model-training)
        - [2.3.1 Data Pre-processing](#231-data-pre-processing)

        - [2.3.2 Network Model Training](#232-network-model-training)

- [3. Exercise](#3-exercise)

    - [3.1 Model Training and Validation](#31-model-training-and-validation)

        - [3.1.1 Data Conversion](#311-data-conversion)

        - [3.1.2 Model Training](#312-model-training)

        - [3.1.3 Model Validation](#313-model-validation)

    - [3.2 Build and Train a Neural Network](#32-build-and-train-a-neural-network)







Goal：

1. To learn the basic elements of Tensorflow and the major building blocks of CNN, and understand the principle of the associated computation 



2. To learn and apply the relevant APIs in Tensorflow



3. To understand the conversion of the standard format `TFrecord` in Tensorflow, and common data pre-processing methods



4. To learn and apply the `slim` module, and build a neural network based on CIFAR-10







## 1. Introduction to CNN

### 1.1 Introduction to TensorFlow

TensorFlow is an end-to-end open source platform for machine learning (ML). It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.



There are three major components of the Tensorflow: **Data Flow Graph**，**Tensor**，**Session**



The core concept of TensorFlow is **Data Flow Graph**. In Tensorflow, data flows from one node to another. The output of the previous node is used as the input (or partial input) of the next node.



**Data Flow Graph** is composed of **Nodes** and **Edges**.

- **Node** represents the operations (**OP**) on the data. It can also be used to represent the source or destination of the data

- **Edge** represents the connection between the nodes



The data flowing through the **Edge** is called **Tensor**. For a neural network that works with images, the common format of data is of the form: `batch*height*width*channel`.

- `batch` represents the sample size for each iteration

- `height` and `weight` represents the spatial size of the each sample

- `channel` represents the number of channels in the input image or feature map. For example, a colored image has 3 channels (R, G, B)



Thus, we easily understand the origin of the name “Tensorflow”, the flow of Tensor among the nodes. 



The data flow graph made of nodes and edges only defines the operations, instead of computing any values. To obtain the computation result, we need to instantiate **session** to evaluate the tensors.



The following graph below illustrates the working principle of Tensorflow.

![](https://i.loli.net/2019/06/11/5cff57e53bb2b20404.gif)

It is a simple regression model. Starting from the input, the data propagates forward layer by layer, with computation done according to the pre-defined weights (w) and biases (b). The first layer is ReLU layer, which contains matrix multiplication (tf.matmul), biased addition (BiasAdd) and non-linear activation (tf.nn.relu). The second layer is a Logit layer. Finally, the SGD trainer calculates the gradient and updates the weights and parameters.



### 1.2 Basic elements of CNN

CNN is a feedforward neural network that usually consists of **convolutional layers + activation functions**, **pooling layers**, and **fully connected layers**. The training of CNN is usually done by back prorogation.



Tensorflow provides high level API for users, so they do not need to write code for the mathematical operations in different layers. The most common API modules are `tf.nn`, `tf.layer` and `tf.contrib`. As we realise there is a lot of functional overlap among the three modules, we will use the most common `tf.nn` module as an example. 



Note that the convolution, activation function, pooling and fully connected layer introduced below are only the **basic operations** of the computational layer. **A computational layer may consist of multiple convolutions, activation functions and poolings**.   



#### 1.2.1 Convolution 

##### 1.2.1.1 Introduction to convolution

For image processing, convolution is an effective way for feature extraction. It aims at extracting and amplifying the feature of the object. The result of an image after convolution is called a **feature map**.



In order to apply convolution, we need to confirm the **step** and **size** of **filter**. Depending on the requirement, sometimes we have to decide if **padding** is needed.



Each filter has 4 dimensions, number of filters (num), height, width, and channel. Height and width are the size of the filter, usually taken as 3 x 3. The depth of the filter has to match with the current feature map, so when designing a filter in a model, we only have to specify num, height, and width.

<img src="https://i.loli.net/2019/06/11/5cff9493c4ab548108.gif" style="zoom:50%">

The above shows a 2-dimensional convolution between a 5x5 feature map and a filter of size 3×3×1, and step =1. The output feature map is of size 3x3.



Size of the output feature map can be computed using the formula below. N represents the size of the output feature map, W is the size of the input feature map, F is the size of the filter, P is the padding number, and S is the step size. 

<img src="https://i.loli.net/2019/06/11/5cff8754ee13e85778.png" style="zoom:50%">



##### 1.2.1.2 Convolution in Tensorflow

We use the most common 2-dimensional convolution API `tf.nn.con2d` as an example, the official documentation can be found in [this link](https://tensorflow.google.cn/api_docs/python/tf/nn/conv2d).



For 2D convolutional API `tf.nn.con2d`, we need to specify parameters including input, filter, step size (strides), and padding method (`same` or `valid`)：

```python

tf.nn.conv2d(

    input,

    filter,

    strides,

    padding,                # 'same' or 'valid'

    use_cudnn_on_gpu=True,

    data_format='NHWC',

    dilations=[1, 1, 1, 1],

    name=None)

```



An example is as follows. First, we have to define the size of the filter. Usually, we will only specify the name, height, width, depth, and channel of the filter, and randomly generate the value in the filter. 

```python

# filter of size 5x5x1 with 32 channels
w = tf.get_variable('weights', [5, 5, 1, 32])               

conv2 = tf.nn.conv2d( [$input] , w, [1, 1, 1, 1], 'SAME')   

```





#### 1.2.2 Pooling

##### 1.2.2.1 Introduction to pooling

**Pooling** is used in CNN to reduce parameters while preserving important information.



The principle of pooling is to perform down-sampling to the data. After partitioning the image, the maximum or average value is used to replace all values in the region. The 2 pooling methods are max pooling and average pooling respectively. The following graph shows the pooling process.

<img src="https://i.loli.net/2019/06/11/5cff94fa530da69044.gif" style="zoom:50%">



##### 1.2.2.2 Pooling in Tensorflow

Take max pooling as an example, the corresponding API in Tensorflow is `tf.nn.max_pool`, the official documentation can be found in [this link](https://tensorflow.google.cn/api_docs/python/tf/nn/max_pool).

```python

tf.nn.max_pool(

    value,

    ksize,             # size of window of every dimension of every tensor input

    strides,           # step size of window of every dimension of every tensor input

    padding,           # 'same' or 'valid'

    data_format='NHWC',

    name=None)

```

An example is shown below,

```python

pool = tf.nn.max_pool(conv2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')

```



#### 1.2.3 Activation Function

##### 1.2.3.1 Introduction to Activation Function

We can learn a straight line to differentiate positive and negative samples with a single layer perceptron, and perform binary classification. Joining multiple perceptrons can draw more straight lines, leading to a more complex classification. However, the union of multiple perceptrons is actually a composition of linear functions, which would also result in a linear function. It cannot handle the more common non-linear problems. 



Therefore, we need to incorporate a non-linear function to allow the linear classifier to solve a non-linear problem.  This is known as the **activation function**. We will use the most common activation function, **ReLU**, as an example and introduce its API.



##### 1.2.3.2 Activation Function in TensorFlow

The API for the activation function ReLU in Tensorflow is `tf.nn.relu`. The official documentation can be found in [this link](https://tensorflow.google.cn/api_docs/python/tf/nn/relu).

```python

tf.nn.relu(

    features,       #tensor input

    name=None)

```

An example is shown below,

```python

activation = tf.nn.relu(pool+b)

```



#### 1.2.4 Fully Connected Layer

##### 1.2.4.1 Introduction to Fully Connected Layer

The above-mentioned convolution, activation, and pooling are used to map the raw data to the **feature space**. The fully connected layer is used to linear map the learnt feature to the sample’s **label space**. 



However, as the fully connected layer uses all the features generated by convolution, it leads to a huge number of parameters, accounting for around 80% of the total parameter of the network. Therefore, some well-performed networks, such as ResNet, usually use **Global Average Pooling** instead of a fully connected layer.



##### 1.2.4.2 Fully Connected Layer in TensorFlow

The API for fully connected layer in Tensorflow is `tf.layer.Dense`, The official documentation can be found in [this link](https://tensorflow.google.cn/api_docs/python/tf/layers/Dense).

```python

tf.layers.dense(

    inputs,

    units,                          #dimension of the output

    activation=None,

    use_bias=True,

    kernel_initializer=None,

    bias_initializer=tf.zeros_initializer(),

    kernel_regularizer=None,

    bias_regularizer=None,

    activity_regularizer=None,

    kernel_constraint=None,

    bias_constraint=None,

    trainable=True,

    name=None,

    reuse=None)

```

An example is shown below,

```

dense = tf.layers.dense(inputs=pool, units=1024, activation=tf.nn.relu)

```



### 1.3 Implementing a Computational Layer

In this experiment, we provided a network that used to identify hand-written numbers, LeNet. (see `def lenet` in `Lab_01_visual.py`)



Take the second computation layer, `conv2`, as an example. It is composed of an activation function `ReLu`, 64 filter kernels of size 5x5x32, and a `Max Pooling` function.   

```python

with tf.variable_scope('conv2'):
        
	w2 = tf.get_variable('weights', [5, 5, 32, 64])
        
	b2 = tf.get_variable('biases', [64], initializer=tf.zeros_initializer())
        
	end_points['conv2'] = tf.nn.relu( tf.nn.conv2d(end_points['pool1'], 
                                          w2, [1, 1, 1, 1], 'SAME') + b2)
    
end_points['pool2'] = tf.nn.max_pool(end_points['conv2'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')

```

As seen in the above code, `conv2` takes the output of the first layer, `pool1`, as the input for convolution, and produces output `pool2`. 







## 2 Introduction to CIFAR-10

[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) is a classic dataset for computer vision. It includes 60000 32x32 coloured images of 10 different types of objects, with 50000 images for training and 10000 for testing.  

![](https://i.loli.net/2019/06/14/5d035d6e06bb364806.jpg)



In the `cifar-10-data` folder, there are 5 `data_batch` and 1 `test_batch`. Each batch contains a python dictionary. In the dictionary, we are interested in the keyword `data` and `label`. 



Every image is stored in a flattened form (a 32x32 image with 3 channels is flattened to a list of length 3072). The data type is uint8. The first 1024 data in the list stores the red channel, the next 1024 data stores the green channel and the last 1024 stores the blue channel. 



### 2.1 Data Conversion

As the data in CIFAR-10 is stored as a list of length 3072, we have to convert it back to the format 32x32x3 to provide usable images for testing for the subsequent model. 



As the recommended format for Tensorflow is `TFrecord`, with the suffix `.record`, we have to further convert the image into `.record` format. Official API from Tensorflow is provided for the conversion to `.record` format. 



Introduction to the `TFrecord` format and the user manual of the API can be found in [this link](https://www.tensorflow.org/tutorials/load_data/tf_records).



Under `/lab_01/` we provided the python file, `convert_tf_record_cifar.py`, for data conversion



First, we read from the binary file and extra the data and label respectively,

```python

# Read from binary file, and extract data and label respectively

fid = open(filename, 'rb')

py_dict = pickle.load(fid, encoding='iso-8859-1')

datas = np.asarray(py_dict['data'], dtype=np.uint8)

labels = np.asarray(py_dict['labels'])

```

Then we convert the data and label to the default data format

```python

# convert data and label to default data format

img = np.zeros((32, 32, 3), dtype=np.uint8)

for ch in range(3):

    img[:, :, ch] = np.reshape(data[ch * 1024: (ch + 1) * 1024], (32, 32))

label = int(labels[i])

# cv2.imwrite('imgs/{}-{:05}.jpg'.format(label, i), img)

```

After that, we further convert the image to TFrecord format. Here, we write the record file according to the standard format of the tfrecord,

```python

# data format declaration

example = tf.train.Example(

    features=tf.train.Features(

        feature={

            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()])),

            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))

        }

    )

)

# write to record file

writer.write(example.SerializeToString())

```



### 2.2 Model Introduction

In this experiment, we defined a `cifar_inference_slim` model. This model can be used for the **classification** of the `CIFAR` dataset. It contains 3 convolution layers, 2 fully connected layers, and 1 pooling layer. The code for the model definition is in line 72-80 of `/lab_01/cifar10.py`. The code is as follow,

```python

def cifar_inference_slim(X, is_training):

    with tf.variable_scope('cifar-10'):

        with slim.arg_scope([slim.batch_norm], is_training=is_training, center=True, scale=True, decay=0.9,epsilon=0.001):

            with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, padding="SAME"):

                with slim.arg_scope([slim.fully_connected], normalizer_fn=slim.batch_norm,

                                    weights_regularizer=slim.l2_regularizer(0.0005)):
                    
                    # convolutional layer 1

                    net = slim.conv2d(X, kernel_size=[5, 5], num_outputs=32, stride=1, scope='conv1')

                    # convolutional layer 2

                    net = slim.conv2d(net, kernel_size=[5, 5], num_outputs=32, stride=1, scope='conv2')

                    # pooling layer

                    net = slim.max_pool2d(net, [3, 3], 2, padding="SAME", scope="conv2/pool")

                    # # convolutional layer 3

                    net = slim.conv2d(net, kernel_size=[3, 3], num_outputs=64, stride=2, scope='conv4')

                    # flatten the multi-dimension data to 1 dimension，usually used between the convolutional layer and fully connected layer

                    net = slim.flatten(net, scope="conv2/flatten")

                    # fully connected layer 1

                    net = slim.fully_connected(net, 192, scope="fc2")

                    #fully connected layer 2

                    net = slim.fully_connected(net, 10, scope="pre_softmax")

    return net

```

As we used the `slim` module in Tensorflow, the layer implementation is different from chapter 1.3 in this tutorial.



The slim library provides multiple API including `data`, `evaluation`, `layer`, `learning`, `losses`, `regularizers`, and `variable`. Further information can be found in the [this link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim).

This tutorial will only focus on some more common API. 



#### 2.2.1 Introduction to the `slim` library

The `slim` library can remove a lot of duplicated template code in Tensorflow, making the code more compact and readable. 



Here is a computational layer without using `slim`.

```python

with tf.variable_scope('conv2'):

    w2 = tf.get_variable('weights', [5, 5, 32, 64])

    b2 = tf.get_variable('biases', [64], initializer=tf.zeros_initializer())

        end_points['conv2'] = tf.nn.relu( tf.nn.conv2d(end_points['pool1'], 

                                          w2, [1, 1, 1, 1], 'SAME') + b2)

    end_points['pool2'] = tf.nn.max_pool(end_points['conv2'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')

```

It is obvious that we only need 1 line of code to define a computational layer using `slim`. `Slim` automatically calculates the dimension of the corresponding `kernel` according to the input `feature map` size, with no user manual effort required.  



Before using the `slim` library, we have to import it as follow,

```python

import tensorflow as tf

import tensorflow.contrib.slim as slim

```



#### 2.2.2 Common `slim` API

##### 2.2.2.1 Using `slim.con2d`to implement a **Convolutional Layer**



The source function of `slim.conv2d`is as follow,

```python

def convolution(inputs, num_outputs, kernel_size, stride=1, padding='SAME',

                data_format=None,

                rate=1,

                activation_fn=nn.relu,

                normalizer_fn=None,

                normalizer_params=None,

                weights_initializer=initializers.xavier_initializer(),

                weights_regularizer=None,

                biases_initializer=init_ops.zeros_initializer(),

                biases_regularizer=None,

                reuse=None,

                variables_collections=None,

                outputs_collections=None,

                trainable=True,

                scope=None)

```

An example is shown below,

```python

input = ...

net = slim.conv2d(input,256,[3,3],strides=1,scope=`conv1`)

#As we do not specify any other parameters，such as stride and activation function，strides=1 and activation=nn.relu are used as default.

```

##### 2.2.2.2 using `slim.max_pool2d` to implement a **Pooling Layer**

The source function of `slim.max_pool2d`is as follow, 

```python

def max_pool2d(inputs, kernel_size, stride=2, padding='VALID',

               data_format=DATA_FORMAT_NHWC,

               outputs_collections=None,

               scope=None)

```

An example is shown below,

```python

input...

net = slim.max_pool2d(input, [3, 3], 2, padding="SAME", scope="conv1/pool")

```

`slim` also provides `slim.avg_pool2d` API, users can check the official document for the source code.



##### 2.2.2.3 Using `slim.fully_connected` to implement a **Fully Connected Layer**

The source function definition of `slim.fully_connected` is as follow, 

```python

def fully_connected(inputs, num_outputs, activation_fn=nn.relu,

                    normalizer_fn=None,

                    normalizer_params=None,

                    weights_initializer=initializers.xavier_initializer(),

                    weights_regularizer=None,

                    biases_initializer=init_ops.zeros_initializer(),

                    biases_regularizer=None,

                    reuse=None,

                    variables_collections=None,

                    outputs_collections=None,

                    trainable=True,

                    scope=None)

```

An example is shown below,

```python

net = slim.fully_connected(net, 10, scope="pre_softmax")

```

##### 2.2.2.4 Applying `slim.arg_scope`

To build a network that contains a lot of identical parameters, e.g. all convolutional layers use 3x3 convolution with step size of 2 and ReLU as activation function, one way is to repeatedly apply `slim.conv2d`. However, the code would be less neat. In this case, we can use `slim.arg.scope` to specify the parameter within the `scope`. 



The source function definition of `slim.arg_scope` is

```python

arg_scope(list_ops_or_scope, **kwargs)

```

`list_ops_or_scope`refers to the list or tuple of operations toast argument scope for. `kwargs`defines the default for each op in the form `keyword=value`. Using one of the `slim.arg_scope` of the model mentioned above as an example, 

```python

with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, padding="SAME"):

```

We define all convolutional layer that uses `slim.conv2d` would use `slim.batch_norm` for normalisation and padding with `SAME`. After specifying the value of these parameters, all convolutional layers within the model that use `slim.conv2d` would use the specified values as default; users do not need to specify the values again. 



### 2.3 Data Pre-processing and Network model Training

After building the model, the next step is to train the model. Before training the model, we have to understand a very important concept, **data pre-processing**. 



#### 2.3.1 Data Pre-processing

There are 2 major reasons for data pre-processing: 1. To enhance the feature of the image. For example, high pass filtering (enhance the edges) and low pass filtering (image smoothening) can improve the accuracy of feature extraction, image segmentation, matching, and identification; 2. To increase the data size. The robustness of the model can be improved by apply inversion, rotation, compression, contrast adjustment, greyscale conversion, etc. to the image.   

<img src="https://i.loli.net/2019/07/17/5d2e9d6a1c0f828836.png" style="zoom:75%" align=center>

In this experiment, the pre-processing applied are random crop, lateral flip, brightness adjustment, and contrast adjustment. 

```pyhton

distorted_image = tf.random_crop(image_float, [24, 24, 3])

distorted_image = tf.image.random_flip_left_right(distorted_image)

distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)

distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)



float_image = tf.image.per_image_standardization(distorted_image)

```

`tf.image` in Tensorflow also provides other API for image processing. Further details can be found in [this link](https://www.tensorflow.org/api_docs/python/tf/image).



#### 2.3.2 Network Model Training

The major code of the training part of this experiment is as follow,

```python

train_loss = _loss(feature, Y)



global_step = tf.train.get_or_create_global_step()

lr = tf.train.exponential_decay(0.01, global_step, num_of_samples_per_decay, 0.5, staircase=False)



 # batch_norm update

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

with tf.control_dependencies(update_ops):

    optimizer = tf.train.MomentumOptimizer(lr, 0.9).minimize(train_loss, global_step=global_step)

```

- `_loss`function is used to measure the difference between the predicted value and ground truth

- `global_step`is used to record the number of executed step. Global step will automatically be increased by 1 for every network parameter update

- `tf.train.exponential_decay` is used to reduce the learning rate exponentially. Detail explanation of the API is provided in the Tensorflow official document

    ```python

    tf.train.exponential_decay(

    learning_rate,

    global_step,

    decay_steps,

    decay_rate,

    staircase=False,

    name=None)

    ```

- `update_ops` is used to update the parameter of the network 

- `optimizer`**momentum** is used here instead of the common SGD. The momentum algorithm borrowed the concept of momentum from physics. It imitates the inertia of object movement by partially retaining the gradients of the past steps. The final update direction is decided by fine-tuning the accumulated gradient using the gradient of the current batch. This algorithm can improve the stability of the network to a certain extent, thus leading faster learning and can escape from the local optimum to a certain degree. Details of the API can be found in the following [link](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)

  >**Note：**
  >
  >A comparison of different optimisation method can be found in the following [link](https://blog.csdn.net/u010089444/article/details/76725843).







## 3. Exercise

### 3.1 Model Training and Validation

There are 3 files in the `lab_01` folder,

- `cifar-10-data`：CIFAR10 data set, contain 5 training `data_batch` and 1 testing `test_batch`

- `convert_tf_record_cifar.py`：data conversion program. It converts the binary image data of CIFAR10 to standard TensorFlow record format

- `cifar.py`：Main program for this exercise



#### 3.1.1 Data Conversion

Open the docker. Go to `cifar_train` folder using command `cd`. Then enter the following command,

```shell

python convert_tf_record_cifar.py

```



The script reads the respective binary file in `./cifar-10-data/` and convert it to tfrecord file.After executing the script, 2 tfrecord files  `cifar10_train.record` and `cifar10_test.record` will be generated.



If the users want to view the training data in image format, they can uncomment line 40 `cv2.imwrite` in `convert_tf_record_cifar.py`



#### 3.1.2 Model Training

The user manual of the script can be view using `python cifar10.py -h`. The major parameters used are as follow,

- `num_decay` defines the number of iterations to reduce the learning rate by half

- `model_type` is for model selection. Here, we provide 2 predefined model structures, `slim` and `large`. The `large` model is to be defined by the user in the next experiment

- `mode` defines if the training mode (`train`) or  testing mode (`test`) is used



Detail commands are as follow,

```shell

python cifar10.py --mode train --gpu 0 --model slim --save_path ./model_slim/

```

After training, the user can use `tensorboard --logdir ${save_path}` to check training related information via the GUI. We have inserted a summary checkpoint in the model to track the important parameters



Command to open the GUI is as follow,

```shell

tensorboard --logdir ./model_slim/

```



After entering the command, there will be a link to tensorboard shown in the terminal. Right-click the link to open the GUI. The interface is as follow,

![tensorboard2.png](https://i.loli.net/2019/03/12/5c8788048ff4a.png)

More detail about the tensorboard is available in the [official website of Tensorflow](https://www.tensorflow.org/guide/summaries_and_tensorboard?hl=zh-cn)。



#### 3.1.3 Model Validation

Use the following command to finish model testing,

```python

python cifar10.py --mode test --gpu 0 --model slim --save_path ./model_slim/

```



This script read the corresponding model file from the ${save path}, and test each image in the testing set, and display the result.



Results are as follow, 

```shell

[INFO]: Starting testing ...

[INFO]: Load Model from model_large/model-ckpt-10000

INFO:tensorflow:Restoring parameters from model_large/model-ckpt-10000

[INFO]: Restoring parameters from model_large/model-ckpt-10000

[INFO]: IMG 1/10000 --> [CORRECT] Prediction: [4]; Gt-label: [4]

[INFO]: IMG 2/10000 --> [CORRECT] Prediction: [6]; Gt-label: [6]

[INFO]: IMG 3/10000 --> [WRONG] Prediction: [0]; Gt-label: [3]

[INFO]: IMG 4/10000 --> [CORRECT] Prediction: [1]; Gt-label: [1]

[INFO]: IMG 5/10000 --> [CORRECT] Prediction: [8]; Gt-label: [8]

```



During testing, we extract the output of the last fully connected layer for every image. It is a 10-dimension output, the same as the number of groups in CIFAR 10 dataset. We use the corresponding label of the largest of the 10 numbers as the prediction result. The result is considered correct if it is the same as ground truth, and vice versa. 



### 3.2 Building and Training a Neural Network

Build a network model in line 94 of `cifar10.py`, requirements are as follow,

- Build a neural network with the following structure, with predefined input X. Users can directly run the script.

![model](https://i.loli.net/2019/07/17/5d2e927d907d517667.png)

- Use the `slim` library to build the model

  - Use the same value as the example for parameters for `batch normalisation`

  - Use `batch normalization` in all convolutional layers，use `SAME` for padding 

  - Use `batch normalization` in all fully connected layers，use `l2_regularizer` for `weights_regularizer`



After building the network model, repeat experiment 3.1 to train the model and validate the result. Compare the result as the depth of the model grows, check if there is any change in the result.



Note，in `cifar10.py`, `large` is the user-define model。In training and validation，uesrs only need to change `model` from `slim` to `large`.

```shell

python cifar10.py --mode train --gpu 0 --model large --save_path ./model_large/

python cifar10.py --mode test --gpu 0 --model large --save_path ./model_large/

```

